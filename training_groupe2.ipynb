{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchvision.datasets import CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print(device)\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[2, 1]\n",
      "[2, 1]\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[2, 1]\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "PreActResNet                             [256, 10]                 --\n",
      "├─Conv2d: 1-1                            [256, 64, 32, 32]         1,728\n",
      "├─Sequential: 1-2                        [256, 64, 32, 32]         --\n",
      "│    └─PreActBlock_0: 2-1                [256, 64, 32, 32]         --\n",
      "│    │    └─BatchNorm2d: 3-1             [256, 64, 32, 32]         128\n",
      "│    │    └─Conv2d: 3-2                  [256, 64, 32, 32]         9,216\n",
      "│    └─PreActBlock_0: 2-2                [256, 64, 32, 32]         --\n",
      "│    │    └─BatchNorm2d: 3-3             [256, 64, 32, 32]         128\n",
      "│    │    └─Conv2d: 3-4                  [256, 64, 32, 32]         9,216\n",
      "├─Sequential: 1-3                        [256, 128, 16, 16]        --\n",
      "│    └─PreActBlock_0: 2-3                [256, 128, 16, 16]        --\n",
      "│    │    └─BatchNorm2d: 3-5             [256, 64, 32, 32]         128\n",
      "│    │    └─Sequential: 3-6              [256, 128, 16, 16]        1,024\n",
      "│    │    └─Conv2d: 3-7                  [256, 128, 16, 16]        9,216\n",
      "│    └─PreActBlock_0: 2-4                [256, 128, 16, 16]        --\n",
      "│    │    └─BatchNorm2d: 3-8             [256, 128, 16, 16]        256\n",
      "│    │    └─Conv2d: 3-9                  [256, 128, 16, 16]        18,432\n",
      "├─Sequential: 1-4                        [256, 256, 8, 8]          --\n",
      "│    └─PreActBlock_0: 2-5                [256, 256, 8, 8]          --\n",
      "│    │    └─BatchNorm2d: 3-10            [256, 128, 16, 16]        256\n",
      "│    │    └─Sequential: 3-11             [256, 256, 8, 8]          2,048\n",
      "│    │    └─Conv2d: 3-12                 [256, 256, 8, 8]          18,432\n",
      "│    └─PreActBlock_0: 2-6                [256, 256, 8, 8]          --\n",
      "│    │    └─BatchNorm2d: 3-13            [256, 256, 8, 8]          512\n",
      "│    │    └─Conv2d: 3-14                 [256, 256, 8, 8]          36,864\n",
      "├─Linear: 1-5                            [256, 10]                 10,250\n",
      "==========================================================================================\n",
      "Total params: 117,834\n",
      "Trainable params: 117,834\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 8.11\n",
      "==========================================================================================\n",
      "Input size (MB): 3.15\n",
      "Forward/backward pass size (MB): 1275.09\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 1278.71\n",
      "==========================================================================================\n",
      "117834\n"
     ]
    }
   ],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# class PreActBottleneck(nn.Module):\n",
    "#     '''Pre-activation version of the original Bottleneck module.'''\n",
    "#     expansion = 4\n",
    "\n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(PreActBottleneck, self).__init__()\n",
    "#         self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(planes)\n",
    "#         self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "#         if stride != 1 or in_planes != self.expansion*planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(x))\n",
    "#         shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "#         out = self.conv1(out)\n",
    "#         out = self.conv2(F.relu(self.bn2(out)))\n",
    "#         out = self.conv3(F.relu(self.bn3(out)))\n",
    "#         out += shortcut\n",
    "#         return out\n",
    "\n",
    "# class PreActBlock(nn.Module):\n",
    "#     '''Pre-activation version of the BasicBlock.'''\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(PreActBlock, self).__init__()\n",
    "#         self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "#         if stride != 1 or in_planes != self.expansion*planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(x))\n",
    "#         shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "#         out = self.conv1(out)\n",
    "#         out = self.conv2(F.relu(self.bn2(out)))\n",
    "#         out += shortcut\n",
    "#         return out\n",
    "class PreActBlock_0(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock_0, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=planes//16)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False, groups=planes//16)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        # out = self.dropout(out)\n",
    "        # out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        # self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(1024*block.expansion, num_classes)\n",
    "        # self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        print(strides)\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def model_1():\n",
    "    return PreActResNet(PreActBlock_0, [2,2,2])\n",
    "\n",
    "def test():\n",
    "    net = model_1()\n",
    "    y = net((torch.randn(512,3,32,32)))\n",
    "\n",
    "test()\n",
    "from torchinfo import summary\n",
    "model = model_1()\n",
    "model.to(device)\n",
    "print(summary(model, input_size=(256,3,32,32),verbose=0))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initial CIFAR10 dataset has 50000 samples\n",
      "Subset of CIFAR10 dataset has 7500 samples\n",
      "num of train batches 30\n",
      "num of test batches 40\n",
      "torch.Size([256, 3, 32, 32]) torch.Size([256, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import default_collate\n",
    "cutmix = v2.CutMix(alpha=1.,num_classes=NUM_CLASSES)\n",
    "# mixup = v2.MixUp(alpha=0.2,num_classes=NUM_CLASSES)\n",
    "# cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return cutmix(*default_collate(batch))\n",
    "\n",
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network. \n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "### The data from CIFAR10 will be downloaded in the following folder\n",
    "rootdir = './data/cifar10'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "## number of target samples for the final dataset\n",
    "num_train_examples = len(c10train)\n",
    "# num_samples_subset = 7500\n",
    "num_samples_subset = num_train_examples\n",
    "\n",
    "\n",
    "## We set a seed manually so as to reproduce the results easily\n",
    "seed  = 2147483647\n",
    "\n",
    "\n",
    "## Generate a list of shuffled indices ; with the fixed seed, the permutation will always be the same, for reproducibility\n",
    "indices = list(range(num_train_examples))\n",
    "np.random.RandomState(seed=seed).shuffle(indices)## modifies the list in place\n",
    "\n",
    "## We define the Subset using the generated indices \n",
    "c10train_subset = torch.utils.data.Subset(c10train,indices[:num_samples_subset])\n",
    "print(f\"Initial CIFAR10 dataset has {len(c10train)} samples\")\n",
    "print(f\"Subset of CIFAR10 dataset has {len(c10train_subset)} samples\")\n",
    "batch_size = 256\n",
    "# Finally we can define anoter dataloader for the training data\n",
    "trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True, collate_fn=collate_fn)\n",
    "testloader = DataLoader(c10test,batch_size=batch_size) \n",
    "trainloader_subset = DataLoader(c10train_subset,batch_size=batch_size,shuffle=True, collate_fn=collate_fn)\n",
    "### You can now use either trainloader (full CIFAR10) or trainloader_subset (subset of CIFAR10) to train your networks.\n",
    "print(\"num of train batches\",len(trainloader_subset))\n",
    "print(\"num of test batches\",len(testloader))\n",
    "x = next(iter(trainloader_subset))\n",
    "print(x[0].shape, x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, val_loader,  optimizer, scheduler, loss_fn, n_epochs=1, save_name=False, plot_losses=False):\n",
    "    start_time = time.time()\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for e in range(n_epochs):\n",
    "        print(\"epoch\", e)\n",
    "        train_loss = 0\n",
    "        for i_batch, batch in enumerate(loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model.forward(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            # print(predicted[0],labels[0])\n",
    "            # print(predicted.shape, labels.shape)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            # if i_batch%10==0: print(\"batch\", i_batch, \"training loss\", loss.item())\n",
    "        train_loss_epoch = train_loss/(i_batch+1)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.inference_mode():\n",
    "            val_loss = 0\n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model.forward(images)\n",
    "                vloss = loss_fn(outputs, labels)\n",
    "                val_loss+= vloss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            val_loss_epoch = val_loss/(i_batch+1)\n",
    "        print(f'accuracy: {100 * correct / total} %')\n",
    "        print(\"correct\",correct)\n",
    "        print(\"total\",total)\n",
    "        print(\"train_loss_epoch\", train_loss_epoch)\n",
    "        print(\"val_loss_epoch\", val_loss_epoch)\n",
    "        training_losses.append(train_loss_epoch)\n",
    "        validation_losses.append(val_loss_epoch)\n",
    "        scheduler.step(val_loss_epoch)\n",
    "\n",
    "    training_time = start_time-time.time()\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # print(pytorch_total_params)\n",
    "    if save_name:\n",
    "        torch.save({\n",
    "            'accuracy': {100 * correct // total},\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'n_epochs': n_epochs,\n",
    "            'batch_size':batch_size,\n",
    "            'loss_fn':loss_fn.__str__,\n",
    "            'total_trainable_params':pytorch_total_params,\n",
    "            'training_time': training_time,\n",
    "            }, \"./checkpoints/\"+save_name)\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(range(n_epochs),validation_losses)\n",
    "        plt.plot(range(n_epochs),training_losses)\n",
    "        plt.legend(['validation loss', 'training loss'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "[2, 1, 1]\n",
      "[2, 1, 1]\n",
      "690122\n",
      "epoch 0\n",
      "accuracy: 37.09 %\n",
      "correct 3709\n",
      "total 10000\n",
      "train_loss_epoch 2.142145586013794\n",
      "val_loss_epoch 1.742912584543228\n",
      "epoch 1\n",
      "accuracy: 41.09 %\n",
      "correct 4109\n",
      "total 10000\n",
      "train_loss_epoch 1.8578971346219382\n",
      "val_loss_epoch 1.6111958473920822\n",
      "epoch 2\n",
      "accuracy: 41.34 %\n",
      "correct 4134\n",
      "total 10000\n",
      "train_loss_epoch 1.8177031795183818\n",
      "val_loss_epoch 1.623967444896698\n",
      "epoch 3\n",
      "accuracy: 45.14 %\n",
      "correct 4514\n",
      "total 10000\n",
      "train_loss_epoch 1.7613359451293946\n",
      "val_loss_epoch 1.5005786895751954\n",
      "epoch 4\n",
      "accuracy: 44.15 %\n",
      "correct 4415\n",
      "total 10000\n",
      "train_loss_epoch 1.8090513745943706\n",
      "val_loss_epoch 1.577159935235977\n",
      "epoch 5\n",
      "accuracy: 49.26 %\n",
      "correct 4926\n",
      "total 10000\n",
      "train_loss_epoch 1.7313177744547525\n",
      "val_loss_epoch 1.4344119250774383\n",
      "epoch 6\n",
      "accuracy: 49.05 %\n",
      "correct 4905\n",
      "total 10000\n",
      "train_loss_epoch 1.5832073450088502\n",
      "val_loss_epoch 1.3555822879076005\n",
      "epoch 7\n",
      "accuracy: 54.17 %\n",
      "correct 5417\n",
      "total 10000\n",
      "train_loss_epoch 1.6039298574129741\n",
      "val_loss_epoch 1.2712558388710022\n",
      "epoch 8\n",
      "accuracy: 50.14 %\n",
      "correct 5014\n",
      "total 10000\n",
      "train_loss_epoch 1.6670109788576761\n",
      "val_loss_epoch 1.4118363797664641\n",
      "epoch 9\n",
      "accuracy: 53.32 %\n",
      "correct 5332\n",
      "total 10000\n",
      "train_loss_epoch 1.6206573724746705\n",
      "val_loss_epoch 1.2961553126573562\n",
      "epoch 10\n",
      "accuracy: 60.6 %\n",
      "correct 6060\n",
      "total 10000\n",
      "train_loss_epoch 1.5485934297243753\n",
      "val_loss_epoch 1.1372774675488473\n",
      "epoch 11\n",
      "accuracy: 59.06 %\n",
      "correct 5906\n",
      "total 10000\n",
      "train_loss_epoch 1.419532561302185\n",
      "val_loss_epoch 1.1688791066408157\n",
      "epoch 12\n",
      "accuracy: 62.21 %\n",
      "correct 6221\n",
      "total 10000\n",
      "train_loss_epoch 1.44629518588384\n",
      "val_loss_epoch 1.0941889151930808\n",
      "epoch 13\n",
      "accuracy: 59.24 %\n",
      "correct 5924\n",
      "total 10000\n",
      "train_loss_epoch 1.5105339646339417\n",
      "val_loss_epoch 1.1683710426092149\n",
      "epoch 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m scheduler \u001b[39m=\u001b[39m ReduceLROnPlateau(optimizer, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     trainloader_subset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     testloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     scheduler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss_fn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     plot_losses\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollate_fn\u001b[39m(batch):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cutmix(\u001b[39m*\u001b[39;49mdefault_collate(batch))\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\v2\\_augment.py:183\u001b[0m, in \u001b[0;36m_BaseMixUpCutMix.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39m# By default, the labels will be False inside needs_transform_list, since they are a torch.Tensor coming\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# after an image or video. However, we need to handle them in _transform, so we make sure to set them to True\u001b[39;00m\n\u001b[0;32m    182\u001b[0m needs_transform_list[\u001b[39mnext\u001b[39m(idx \u001b[39mfor\u001b[39;00m idx, inpt \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(flat_inputs) \u001b[39mif\u001b[39;00m inpt \u001b[39mis\u001b[39;00m labels)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(inpt, params) \u001b[39mif\u001b[39;00m needs_transform \u001b[39melse\u001b[39;00m inpt\n\u001b[0;32m    185\u001b[0m     \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[0;32m    186\u001b[0m ]\n\u001b[0;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\v2\\_augment.py:184\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39m# By default, the labels will be False inside needs_transform_list, since they are a torch.Tensor coming\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# after an image or video. However, we need to handle them in _transform, so we make sure to set them to True\u001b[39;00m\n\u001b[0;32m    182\u001b[0m needs_transform_list[\u001b[39mnext\u001b[39m(idx \u001b[39mfor\u001b[39;00m idx, inpt \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(flat_inputs) \u001b[39mif\u001b[39;00m inpt \u001b[39mis\u001b[39;00m labels)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    183\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;00m needs_transform \u001b[39melse\u001b[39;00m inpt\n\u001b[0;32m    185\u001b[0m     \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[0;32m    186\u001b[0m ]\n\u001b[0;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\v2\\_augment.py:317\u001b[0m, in \u001b[0;36mCutMix._transform\u001b[1;34m(self, inpt, params)\u001b[0m\n\u001b[0;32m    315\u001b[0m x1, y1, x2, y2 \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mbox\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    316\u001b[0m rolled \u001b[39m=\u001b[39m inpt\u001b[39m.\u001b[39mroll(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 317\u001b[0m output \u001b[39m=\u001b[39m inpt\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m    318\u001b[0m output[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, y1:y2, x1:x2] \u001b[39m=\u001b[39m rolled[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, y1:y2, x1:x2]\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inpt, (tv_tensors\u001b[39m.\u001b[39mImage, tv_tensors\u001b[39m.\u001b[39mVideo)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = model_1()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=3, min_lr=1e-8, verbose=True)\n",
    "train(\n",
    "    model,\n",
    "    trainloader_subset,\n",
    "    testloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_fn,\n",
    "    n_epochs=150,\n",
    "    plot_losses=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
