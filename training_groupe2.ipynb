{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torchvision.transforms as transforms\n",
    "import torch \n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        # self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
    "        # self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "def model_1():\n",
    "    return PreActResNet(PreActBlock, [1,1,1,1])\n",
    "def test():\n",
    "    net = PreActResNet18()\n",
    "    y = net((torch.randn(512,3,32,32)))\n",
    "    print(y.size())\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initial CIFAR10 dataset has 50000 samples\n",
      "Subset of CIFAR10 dataset has 50000 samples\n",
      "num of train batches 196\n",
      "num of test batches 40\n",
      "torch.Size([256, 3, 32, 32]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network. \n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "### The data from CIFAR10 will be downloaded in the following folder\n",
    "rootdir = './data/cifar10'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "## number of target samples for the final dataset\n",
    "num_train_examples = len(c10train)\n",
    "# num_samples_subset = 7500\n",
    "num_samples_subset = num_train_examples\n",
    "\n",
    "\n",
    "## We set a seed manually so as to reproduce the results easily\n",
    "seed  = 2147483647\n",
    "\n",
    "\n",
    "## Generate a list of shuffled indices ; with the fixed seed, the permutation will always be the same, for reproducibility\n",
    "indices = list(range(num_train_examples))\n",
    "np.random.RandomState(seed=seed).shuffle(indices)## modifies the list in place\n",
    "\n",
    "## We define the Subset using the generated indices \n",
    "c10train_subset = torch.utils.data.Subset(c10train,indices[:num_samples_subset])\n",
    "print(f\"Initial CIFAR10 dataset has {len(c10train)} samples\")\n",
    "print(f\"Subset of CIFAR10 dataset has {len(c10train_subset)} samples\")\n",
    "batch_size = 256\n",
    "# Finally we can define anoter dataloader for the training data\n",
    "trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=batch_size) \n",
    "trainloader_subset = DataLoader(c10train_subset,batch_size=batch_size,shuffle=True)\n",
    "### You can now use either trainloader (full CIFAR10) or trainloader_subset (subset of CIFAR10) to train your networks.\n",
    "print(\"num of train batches\",len(trainloader_subset))\n",
    "print(\"num of test batches\",len(testloader))\n",
    "x = next(iter(trainloader))\n",
    "print(x[0].shape, x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, val_loader,  optimizer, scheduler, loss_fn, n_epochs=1, save_name=False, plot_losses=False):\n",
    "    start_time = time.time()\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for e in range(n_epochs):\n",
    "        print(\"epoch\", e)\n",
    "        train_loss = 0\n",
    "        for i_batch, batch in enumerate(loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model.forward(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if i_batch%10==0: print(\"batch\", i_batch, \"training loss\", loss.item())\n",
    "        train_loss_epoch = train_loss/(i_batch+1)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.inference_mode():\n",
    "            val_loss = 0\n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model.forward(images)\n",
    "                vloss = loss_fn(outputs, labels)\n",
    "                val_loss+= vloss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            val_loss_epoch = val_loss/(i_batch+1)\n",
    "        print(f'accuracy: {100 * correct // total} %')\n",
    "        print(\"train_loss_epoch\", train_loss_epoch)\n",
    "        print(\"val_loss_epoch\", val_loss_epoch)\n",
    "        training_losses.append(train_loss_epoch)\n",
    "        validation_losses.append(val_loss_epoch)\n",
    "        scheduler.step(val_loss_epoch)\n",
    "\n",
    "    training_time = start_time-time.time()\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # print(pytorch_total_params)\n",
    "    if save_name:\n",
    "        torch.save({\n",
    "            'accuracy': {100 * correct // total},\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'n_epochs': n_epochs,\n",
    "            'batch_size':batch_size,\n",
    "            'loss_fn':loss_fn.__str__,\n",
    "            'total_trainable_params':pytorch_total_params,\n",
    "            'training_time': training_time,\n",
    "            }, \"./checkpoints/\"+save_name)\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(range(n_epochs),validation_losses)\n",
    "        plt.plot(range(n_epochs),training_losses)\n",
    "        plt.legend(['validation loss', 'training loss'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "PreActResNet                             [32, 10]                  --\n",
      "├─Conv2d: 1-1                            [32, 64, 32, 32]          1,728\n",
      "├─Sequential: 1-2                        [32, 32, 32, 32]          --\n",
      "│    └─PreActBlock: 2-1                  [32, 32, 32, 32]          --\n",
      "│    │    └─BatchNorm2d: 3-1             [32, 64, 32, 32]          128\n",
      "│    │    └─Sequential: 3-2              [32, 32, 32, 32]          2,048\n",
      "│    │    └─Conv2d: 3-3                  [32, 32, 32, 32]          18,432\n",
      "│    │    └─BatchNorm2d: 3-4             [32, 32, 32, 32]          64\n",
      "│    │    └─Conv2d: 3-5                  [32, 32, 32, 32]          9,216\n",
      "├─Sequential: 1-3                        [32, 64, 16, 16]          --\n",
      "│    └─PreActBlock: 2-2                  [32, 64, 16, 16]          --\n",
      "│    │    └─BatchNorm2d: 3-6             [32, 32, 32, 32]          64\n",
      "│    │    └─Sequential: 3-7              [32, 64, 16, 16]          2,048\n",
      "│    │    └─Conv2d: 3-8                  [32, 64, 16, 16]          18,432\n",
      "│    │    └─BatchNorm2d: 3-9             [32, 64, 16, 16]          128\n",
      "│    │    └─Conv2d: 3-10                 [32, 64, 16, 16]          36,864\n",
      "├─Sequential: 1-4                        [32, 128, 8, 8]           --\n",
      "│    └─PreActBlock: 2-3                  [32, 128, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-11            [32, 64, 16, 16]          128\n",
      "│    │    └─Sequential: 3-12             [32, 128, 8, 8]           8,192\n",
      "│    │    └─Conv2d: 3-13                 [32, 128, 8, 8]           73,728\n",
      "│    │    └─BatchNorm2d: 3-14            [32, 128, 8, 8]           256\n",
      "│    │    └─Conv2d: 3-15                 [32, 128, 8, 8]           147,456\n",
      "├─Sequential: 1-5                        [32, 256, 4, 4]           --\n",
      "│    └─PreActBlock: 2-4                  [32, 256, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-16            [32, 128, 8, 8]           256\n",
      "│    │    └─Sequential: 3-17             [32, 256, 4, 4]           32,768\n",
      "│    │    └─Conv2d: 3-18                 [32, 256, 4, 4]           294,912\n",
      "│    │    └─BatchNorm2d: 3-19            [32, 256, 4, 4]           512\n",
      "│    │    └─Conv2d: 3-20                 [32, 256, 4, 4]           589,824\n",
      "├─Linear: 1-6                            [32, 10]                  2,570\n",
      "==========================================================================================\n",
      "Total params: 1,239,754\n",
      "Trainable params: 1,239,754\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.44\n",
      "==========================================================================================\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 111.15\n",
      "Params size (MB): 4.96\n",
      "Estimated Total Size (MB): 116.50\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = model_1()\n",
    "\n",
    "batch_size = 32\n",
    "print(summary(model, input_size=(batch_size,3,32,32),verbose=0))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "batch 0 training loss 2.3407514095306396\n",
      "batch 10 training loss 2.2884936332702637\n",
      "batch 20 training loss 2.2468039989471436\n",
      "batch 30 training loss 2.236316204071045\n",
      "batch 40 training loss 2.2050962448120117\n",
      "batch 50 training loss 2.1896650791168213\n",
      "batch 60 training loss 2.133185386657715\n",
      "batch 70 training loss 2.1480026245117188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     trainloader_subset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     testloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss_fn,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     plot_losses\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], F_pil\u001b[39m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=)\n",
    "train(\n",
    "    model,\n",
    "    trainloader_subset,\n",
    "    testloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_fn,\n",
    "    n_epochs=10,\n",
    "    plot_losses=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
