{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torchvision.transforms as transforms\n",
    "import torch \n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PreActResNet' object has no attribute 'layer4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     y \u001b[39m=\u001b[39m net((torch\u001b[39m.\u001b[39mrandn(\u001b[39m512\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m32\u001b[39m,\u001b[39m32\u001b[39m)))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39msize())\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m test()\n",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m():\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     net \u001b[39m=\u001b[39m PreActResNet18()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     y \u001b[39m=\u001b[39m net((torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m512\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m32\u001b[39;49m,\u001b[39m32\u001b[39;49m)))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(out)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mavg_pool2d(out, \u001b[39m4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#W2sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PreActResNet' object has no attribute 'layer4'"
     ]
    }
   ],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        # self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "\n",
    "def PreActResNet34():\n",
    "    return PreActResNet(PreActBlock, [3,4,6,3])\n",
    "\n",
    "def PreActResNet50():\n",
    "    return PreActResNet(PreActBottleneck, [3,4,6,3])\n",
    "\n",
    "def PreActResNet101():\n",
    "    return PreActResNet(PreActBottleneck, [3,4,23,3])\n",
    "\n",
    "def PreActResNet152():\n",
    "    return PreActResNet(PreActBottleneck, [3,8,36,3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = PreActResNet18()\n",
    "    y = net((torch.randn(512,3,32,32)))\n",
    "    print(y.size())\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initial CIFAR10 dataset has 50000 samples\n",
      "Subset of CIFAR10 dataset has 50000 samples\n",
      "num of train batches 196\n",
      "num of test batches 40\n",
      "torch.Size([256, 3, 32, 32]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network. \n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "### The data from CIFAR10 will be downloaded in the following folder\n",
    "rootdir = './data/cifar10'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "## number of target samples for the final dataset\n",
    "num_train_examples = len(c10train)\n",
    "# num_samples_subset = 7500\n",
    "num_samples_subset = num_train_examples\n",
    "\n",
    "\n",
    "## We set a seed manually so as to reproduce the results easily\n",
    "seed  = 2147483647\n",
    "\n",
    "\n",
    "## Generate a list of shuffled indices ; with the fixed seed, the permutation will always be the same, for reproducibility\n",
    "indices = list(range(num_train_examples))\n",
    "np.random.RandomState(seed=seed).shuffle(indices)## modifies the list in place\n",
    "\n",
    "## We define the Subset using the generated indices \n",
    "c10train_subset = torch.utils.data.Subset(c10train,indices[:num_samples_subset])\n",
    "print(f\"Initial CIFAR10 dataset has {len(c10train)} samples\")\n",
    "print(f\"Subset of CIFAR10 dataset has {len(c10train_subset)} samples\")\n",
    "batch_size = 256\n",
    "# Finally we can define anoter dataloader for the training data\n",
    "trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=batch_size) \n",
    "trainloader_subset = DataLoader(c10train_subset,batch_size=batch_size,shuffle=True)\n",
    "### You can now use either trainloader (full CIFAR10) or trainloader_subset (subset of CIFAR10) to train your networks.\n",
    "print(\"num of train batches\",len(trainloader_subset))\n",
    "print(\"num of test batches\",len(testloader))\n",
    "x = next(iter(trainloader))\n",
    "print(x[0].shape, x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, val_loader,  optimizer, loss_fn, n_epochs=1, save_name=False, plot_losses=False):\n",
    "    start_time = time.time()\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for e in range(n_epochs):\n",
    "        print(\"epoch\", e)\n",
    "        train_loss = 0\n",
    "        for i_batch, batch in enumerate(loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model.forward(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if i_batch%10==0: print(\"batch\", i_batch, \"training loss\", loss.item())\n",
    "        train_loss_epoch = train_loss/(i_batch+1)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.inference_mode():\n",
    "            val_loss = 0\n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model.forward(images)\n",
    "                vloss = loss_fn(outputs, labels)\n",
    "                val_loss+= vloss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            val_loss_epoch = val_loss/(i_batch+1)\n",
    "        print(f'accuracy: {100 * correct // total} %')\n",
    "        print(\"train_loss_epoch\", train_loss_epoch)\n",
    "        print(\"val_loss_epoch\", val_loss_epoch)\n",
    "        training_losses.append(train_loss_epoch)\n",
    "        validation_losses.append(val_loss_epoch)\n",
    "\n",
    "    training_time = start_time-time.time()\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # print(pytorch_total_params)\n",
    "    if save_name:\n",
    "        torch.save({\n",
    "            'accuracy': {100 * correct // total},\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'n_epochs': n_epochs,\n",
    "            'batch_size':batch_size,\n",
    "            'loss_fn':loss_fn.__str__,\n",
    "            'total_trainable_params':pytorch_total_params,\n",
    "            'training_time': training_time,\n",
    "            }, \"./checkpoints/\"+save_name)\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(range(n_epochs),validation_losses)\n",
    "        plt.plot(range(n_epochs),training_losses)\n",
    "        plt.legend(['validation loss', 'training loss'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "PreActResNet                             [32, 10]                  --\n",
      "├─Conv2d: 1-1                            [32, 64, 32, 32]          1,728\n",
      "├─Sequential: 1-2                        [32, 64, 32, 32]          --\n",
      "│    └─PreActBlock: 2-1                  [32, 64, 32, 32]          --\n",
      "│    │    └─BatchNorm2d: 3-1             [32, 64, 32, 32]          128\n",
      "│    │    └─Conv2d: 3-2                  [32, 64, 32, 32]          36,864\n",
      "│    │    └─BatchNorm2d: 3-3             [32, 64, 32, 32]          128\n",
      "│    │    └─Conv2d: 3-4                  [32, 64, 32, 32]          36,864\n",
      "│    └─PreActBlock: 2-2                  [32, 64, 32, 32]          --\n",
      "│    │    └─BatchNorm2d: 3-5             [32, 64, 32, 32]          128\n",
      "│    │    └─Conv2d: 3-6                  [32, 64, 32, 32]          36,864\n",
      "│    │    └─BatchNorm2d: 3-7             [32, 64, 32, 32]          128\n",
      "│    │    └─Conv2d: 3-8                  [32, 64, 32, 32]          36,864\n",
      "├─Sequential: 1-3                        [32, 128, 16, 16]         --\n",
      "│    └─PreActBlock: 2-3                  [32, 128, 16, 16]         --\n",
      "│    │    └─BatchNorm2d: 3-9             [32, 64, 32, 32]          128\n",
      "│    │    └─Sequential: 3-10             [32, 128, 16, 16]         8,192\n",
      "│    │    └─Conv2d: 3-11                 [32, 128, 16, 16]         73,728\n",
      "│    │    └─BatchNorm2d: 3-12            [32, 128, 16, 16]         256\n",
      "│    │    └─Conv2d: 3-13                 [32, 128, 16, 16]         147,456\n",
      "│    └─PreActBlock: 2-4                  [32, 128, 16, 16]         --\n",
      "│    │    └─BatchNorm2d: 3-14            [32, 128, 16, 16]         256\n",
      "│    │    └─Conv2d: 3-15                 [32, 128, 16, 16]         147,456\n",
      "│    │    └─BatchNorm2d: 3-16            [32, 128, 16, 16]         256\n",
      "│    │    └─Conv2d: 3-17                 [32, 128, 16, 16]         147,456\n",
      "├─Sequential: 1-4                        [32, 256, 8, 8]           --\n",
      "│    └─PreActBlock: 2-5                  [32, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-18            [32, 128, 16, 16]         256\n",
      "│    │    └─Sequential: 3-19             [32, 256, 8, 8]           32,768\n",
      "│    │    └─Conv2d: 3-20                 [32, 256, 8, 8]           294,912\n",
      "│    │    └─BatchNorm2d: 3-21            [32, 256, 8, 8]           512\n",
      "│    │    └─Conv2d: 3-22                 [32, 256, 8, 8]           589,824\n",
      "│    └─PreActBlock: 2-6                  [32, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-23            [32, 256, 8, 8]           512\n",
      "│    │    └─Conv2d: 3-24                 [32, 256, 8, 8]           589,824\n",
      "│    │    └─BatchNorm2d: 3-25            [32, 256, 8, 8]           512\n",
      "│    │    └─Conv2d: 3-26                 [32, 256, 8, 8]           589,824\n",
      "├─Sequential: 1-5                        [32, 512, 4, 4]           --\n",
      "│    └─PreActBlock: 2-7                  [32, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-27            [32, 256, 8, 8]           512\n",
      "│    │    └─Sequential: 3-28             [32, 512, 4, 4]           131,072\n",
      "│    │    └─Conv2d: 3-29                 [32, 512, 4, 4]           1,179,648\n",
      "│    │    └─BatchNorm2d: 3-30            [32, 512, 4, 4]           1,024\n",
      "│    │    └─Conv2d: 3-31                 [32, 512, 4, 4]           2,359,296\n",
      "│    └─PreActBlock: 2-8                  [32, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-32            [32, 512, 4, 4]           1,024\n",
      "│    │    └─Conv2d: 3-33                 [32, 512, 4, 4]           2,359,296\n",
      "│    │    └─BatchNorm2d: 3-34            [32, 512, 4, 4]           1,024\n",
      "│    │    └─Conv2d: 3-35                 [32, 512, 4, 4]           2,359,296\n",
      "├─Linear: 1-6                            [32, 10]                  5,130\n",
      "==========================================================================================\n",
      "Total params: 11,171,146\n",
      "Trainable params: 11,171,146\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 17.77\n",
      "==========================================================================================\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 297.80\n",
      "Params size (MB): 44.68\n",
      "Estimated Total Size (MB): 342.88\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = PreActResNet18()\n",
    "\n",
    "batch_size = 32\n",
    "print(summary(model, input_size=(batch_size,3,32,32),verbose=0))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "batch 0 training loss 1.4943238496780396\n",
      "batch 10 training loss 1.4536267518997192\n",
      "batch 20 training loss 1.5135066509246826\n",
      "batch 30 training loss 1.4765115976333618\n",
      "batch 40 training loss 1.414063572883606\n",
      "batch 50 training loss 1.5518561601638794\n",
      "batch 60 training loss 1.395208716392517\n",
      "batch 70 training loss 1.3852494955062866\n",
      "batch 80 training loss 1.3087797164916992\n",
      "batch 90 training loss 1.3405588865280151\n",
      "batch 100 training loss 1.425873041152954\n",
      "batch 110 training loss 1.5227285623550415\n",
      "batch 120 training loss 1.4172905683517456\n",
      "batch 130 training loss 1.3559699058532715\n",
      "batch 140 training loss 1.4702805280685425\n",
      "batch 150 training loss 1.4040594100952148\n",
      "batch 160 training loss 1.4444127082824707\n",
      "batch 170 training loss 1.3632643222808838\n",
      "batch 180 training loss 1.421619176864624\n",
      "batch 190 training loss 1.423327922821045\n",
      "accuracy: 43 %\n",
      "train_loss_epoch 1.423907338356485\n",
      "val_loss_epoch 1.331356519460678\n",
      "epoch 1\n",
      "batch 0 training loss 1.2819023132324219\n",
      "batch 10 training loss 1.3140015602111816\n",
      "batch 20 training loss 1.3756577968597412\n",
      "batch 30 training loss 1.4336843490600586\n",
      "batch 40 training loss 1.3607172966003418\n",
      "batch 50 training loss 1.3979363441467285\n",
      "batch 60 training loss 1.2970542907714844\n",
      "batch 70 training loss 1.2814421653747559\n",
      "batch 80 training loss 1.2860687971115112\n",
      "batch 90 training loss 1.3423348665237427\n",
      "batch 100 training loss 1.3805797100067139\n",
      "batch 110 training loss 1.280922293663025\n",
      "batch 120 training loss 1.3935654163360596\n",
      "batch 130 training loss 1.2415096759796143\n",
      "batch 140 training loss 1.275942087173462\n",
      "batch 150 training loss 1.396823763847351\n",
      "batch 160 training loss 1.3475239276885986\n",
      "batch 170 training loss 1.2446690797805786\n",
      "batch 180 training loss 1.3127503395080566\n",
      "batch 190 training loss 1.2340563535690308\n",
      "accuracy: 50 %\n",
      "train_loss_epoch 1.319411028404625\n",
      "val_loss_epoch 1.2836775928735733\n",
      "epoch 2\n",
      "batch 0 training loss 1.2932462692260742\n",
      "batch 10 training loss 1.248919129371643\n",
      "batch 20 training loss 1.259337067604065\n",
      "batch 30 training loss 1.2224469184875488\n",
      "batch 40 training loss 1.2854210138320923\n",
      "batch 50 training loss 1.2474651336669922\n",
      "batch 60 training loss 1.21233069896698\n",
      "batch 70 training loss 1.2971574068069458\n",
      "batch 80 training loss 1.2421069145202637\n",
      "batch 90 training loss 1.3015947341918945\n",
      "batch 100 training loss 1.3600949048995972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     trainloader_subset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     testloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss_fn,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     plot_losses\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\training_groupe2.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antoi/ensta/annee_2/sysemb/training_groupe2.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ensta\\annee_2\\sysemb\\.venv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:917\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    913\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 917\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m    919\u001b[0m dtype \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    920\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "train(\n",
    "    model,\n",
    "    trainloader_subset,\n",
    "    testloader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    n_epochs=10,\n",
    "    plot_losses=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
